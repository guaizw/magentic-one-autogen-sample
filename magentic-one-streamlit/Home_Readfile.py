import streamlit as st
import asyncio
import time
import os
from dotenv import load_dotenv
from autogen_ext.models.openai import OpenAIChatCompletionClient, AzureOpenAIChatCompletionClient
from autogen_ext.teams.magentic_one import MagenticOne
from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor
# from autogen.code_utils import create_virtual_env
import sys

load_dotenv()

def format_source_display(source):
    """
    Converts a source identifier into a user-friendly display string with an appropriate emoji.
    
    Args:
        source (str): The message source identifier
        
    Returns:
        str: Formatted string with emoji representing the source
    """
    if source == "user":
        return "üë§ User"
    elif source == "MagenticOneOrchestrator":
        return "ü§ñ MagenticOneOrchestrator"
    elif source == "WebSurfer":
        return "üåê WebSurfer"
    elif source == "FileSurfer":
        return "üìÅ FileSurfer"
    elif source == "Coder":
        return "üíª Coder"
    else:
        return f"üíª Terminal"

async def run_task(user_prompt: str, USE_AOAI, model_name=None):
    """
    Executes a task with the given user prompt using either Azure OpenAI or OpenAI.
    Streams and displays results in the Streamlit UI as they become available.
    
    Args:
        user_prompt (str): The task prompt from the user
        USE_AOAI (bool): Whether to use Azure OpenAI API
        
    Yields:
        Various message chunks and task results
    """
    start_time = time.time()
    if(USE_AOAI):
        client = AzureOpenAIChatCompletionClient(
            azure_endpoint=os.getenv('AZURE_OPEN_AI_ENDPOINT'),
            model=model_name,
            api_version="2024-12-01-preview",
            api_key=os.getenv('AZURE_OPEN_AI_KEY')
        )
    else:
        client = OpenAIChatCompletionClient(
            model=os.getenv('OPEN_AI_MODEL_NAME'),
            api_key=os.getenv('OPEN_AI_API_KEY')
        )
    # venv_dir = "..\.venv"
    # venv_context = create_virtual_env(venv_dir)
    m1 = MagenticOne(client=client, code_executor=LocalCommandLineCodeExecutor())
    
    custom_task = """
    You are an agent helping A*STAR Staff with matters related to overseas travel.
    Please take reference from the files in the folder "references".
    You may use the function readfile.py in the folder "funct" to read the files in "references" if you need them.
    Please note that for a typical overseas travel application, he will need to raise the travel and budget request from A*STAR, and arrange for a quote from Pricebreakers.
    Based on the user's prompt, help to write a template email to the Pricebreaker Agent, with a guide to staff on what are the steps to take.
    If the user prompt is unrelated to overseas travel for A*STAR Staff, do not proceed and ask for a new prompt.
    User Prompt : {user_prompt}
    """


    async for chunk in m1.run_stream(task=custom_task.format(user_prompt=user_prompt)):
        if chunk.__class__.__name__ != 'TaskResult':
            st.write(f"**{format_source_display(chunk.source)}**")
            if chunk.type == 'MultiModalMessage':
                image = 'data:image/png;base64,' + chunk.content[1].to_base64()
                st.image(image)
            else:
                st.markdown(chunk.content)
        else:
            st.write(f"**Task completed in {(time.time() - start_time):.2f} s.**")
        yield chunk
    yield None, time.time() - start_time

async def collect_results(user_prompt: str, USE_AOAI, model_name=None):
    """
    Collects all results from run_task and accumulates token usage statistics.
    Updates session state with token counts.
    
    Args:
        user_prompt (str): The task prompt from the user
        USE_AOAI (bool): Whether to use Azure OpenAI API
        
    Returns:
        list: Collection of all result chunks
    """
    results = []
    async for chunk in run_task(user_prompt, USE_AOAI, model_name):
        results.append(chunk)
    for result in results:
        if result is not None and result.__class__.__name__ == 'TaskResult':
            print(result)
            for message in result.messages:
                if message.source != "user":
                    if message.models_usage:
                        st.session_state.prompt_token = message.models_usage.prompt_tokens + st.session_state.prompt_token
                        st.session_state.completion_token = message.models_usage.completion_tokens + st.session_state.completion_token
    return results

def main():
    st.title('üß†ü§ñ Magentic-One Demo')
    st.write('Implementation using Autogen and Streamlit')

    st.sidebar.title('Settings')
    USE_AOAI = st.sidebar.checkbox("Use Azure OpenAI", value=True)

    if(USE_AOAI):
        aoai_model_options = ["gpt-4o", "gpt-4o-mini", "o3-mini"]
        selected_model = st.sidebar.selectbox("Select Model", aoai_model_options)

    if 'output' not in st.session_state:
        st.session_state.output = None
        st.session_state.elapsed = None
        st.session_state.prompt_token = 0
        st.session_state.completion_token = 0

    prompt = st.text_input('What is the task today?', value='')

    if st.button('Execute'):
        st.write(f"**Task is submitted with {selected_model} model.**")
        results = asyncio.run(collect_results(prompt, USE_AOAI, selected_model))
        st.session_state.elapsed = results[-1][1] if results[-1] is not None else None

    if st.session_state.elapsed is not None:
        st.write(f"**Prompt tokens: {st.session_state.prompt_token}**")
        st.write(f"**Completion tokens: {st.session_state.completion_token}**")

if __name__ == "__main__":
    main()